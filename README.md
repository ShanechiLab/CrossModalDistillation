# Cross-modal knowledge distillation 
Cross-modal knowledge distillation from multi-session spike models to LFP models to develop LFP models with enhanced representational power. Code will be released soon! 

Please check out our manuscript for full results and details.

## Publication:
[Erturk, E., Hashemi, S., Shanechi, M. M. Cross-Modal Representational Knowledge Distillation for Enhanced Spike-informed LFP Modeling. In Advances in Neural Information Processing Systems 2025.](https://openreview.net/forum?id=hT7Nj7SAQb)

## Licence:
Copyright (c) 2025 University of Southern California <br />
See full notice in [LICENSE.md](LICENSE.md) <br />
Eray Erturk, Saba Hashemi, and Maryam M. Shanechi <br />
Shanechi Lab, University of Southern California <br />
